{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3xWang.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Cym0EB5cvRnU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":170},"outputId":"363aeaa4-6ceb-48cb-cde3-62ef8b08d9cc","executionInfo":{"status":"ok","timestamp":1570041385486,"user_tz":-330,"elapsed":28223,"user":{"displayName":"Jovian Jaison","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDD_VAnP3vWvojEh9tY5y5Xue8uiFD4JaqbNzzXjIs=s64","userId":"06808514560552349214"}}},"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive\n","%cd My Drive\n","%cd wang"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /gdrive\n","/gdrive\n","/gdrive/My Drive\n","/gdrive/My Drive/wang\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0aaMQKR3vfO-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"ada3f23d-b6a0-4089-9343-ede169020928","executionInfo":{"status":"ok","timestamp":1570041899377,"user_tz":-330,"elapsed":507775,"user":{"displayName":"Jovian Jaison","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDD_VAnP3vWvojEh9tY5y5Xue8uiFD4JaqbNzzXjIs=s64","userId":"06808514560552349214"}}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","\n","keras = tf.keras\n","import os\n","import math\n","from math import floor\n","import cv2\n","\n","for filename in os.listdir():\n","    if filename.endswith(\".jpg\"): \n","      continue\n","    else:\n","        continue\n","\n","all_image_paths = [str(filename) for filename in os.listdir()]\n","image_count = len(all_image_paths)\n","print(image_count)\n","all_image_paths.sort()\n","filenames = [int(filename.replace('.jpg','')) for filename in os.listdir()]\n","print(filenames)\n","filenames.sort()\n","\n","x = []\n","y = []\n","WIDTH = 256\n","HEIGHT = 256\n","for img in filenames:\n","    print(img)\n","    label = img//100\n","    y.append(label)\n","    \n","    full_size_image = cv2.imread(str(img)+'.jpg')\n","    x.append(cv2.resize(full_size_image, (WIDTH,HEIGHT), interpolation=cv2.INTER_CUBIC))\n","\n","import sklearn\n","from sklearn.model_selection import train_test_split\n","\n","X=np.array(x)\n","X=X/255.0\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2)\n","\n","# Encode labels to hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\n","from keras.utils.np_utils import to_categorical\n","y_trainHot = to_categorical(Y_train, num_classes = 10)\n","y_testHot = to_categorical(Y_test, num_classes = 10)\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","1000\n","[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 98, 93, 91, 96, 94, 95, 92, 97, 99, 100, 101, 102, 103, 104, 106, 105, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 119, 118, 120, 122, 121, 123, 124, 126, 125, 127, 128, 129, 130, 131, 132, 133, 134, 137, 135, 136, 138, 139, 140, 141, 142, 143, 145, 144, 146, 149, 148, 147, 150, 152, 151, 153, 155, 154, 156, 157, 159, 158, 160, 161, 162, 163, 164, 165, 166, 169, 168, 167, 170, 171, 172, 173, 174, 176, 177, 178, 179, 175, 180, 182, 181, 183, 184, 185, 187, 186, 189, 188, 190, 191, 192, 193, 195, 194, 196, 197, 199, 198, 200, 201, 202, 203, 204, 205, 206, 207, 209, 208, 217, 216, 214, 212, 213, 215, 219, 220, 223, 221, 225, 222, 224, 226, 228, 229, 227, 230, 233, 231, 232, 238, 236, 235, 237, 239, 234, 240, 242, 243, 241, 246, 247, 244, 245, 248, 249, 250, 254, 252, 253, 251, 256, 255, 257, 259, 258, 260, 263, 261, 262, 267, 265, 269, 264, 266, 268, 270, 274, 271, 272, 273, 276, 279, 275, 278, 277, 281, 282, 280, 283, 285, 287, 288, 286, 284, 289, 290, 292, 293, 291, 297, 298, 294, 296, 295, 299, 300, 301, 303, 302, 304, 307, 305, 306, 309, 308, 310, 313, 312, 311, 314, 315, 318, 316, 319, 317, 320, 321, 322, 323, 324, 327, 325, 326, 329, 328, 330, 332, 331, 334, 336, 335, 337, 333, 338, 339, 340, 342, 341, 345, 346, 344, 348, 343, 347, 349, 350, 352, 353, 351, 358, 359, 355, 354, 357, 356, 360, 362, 361, 363, 367, 366, 368, 365, 369, 364, 370, 373, 372, 371, 377, 374, 376, 375, 378, 379, 380, 381, 383, 382, 388, 384, 385, 386, 389, 387, 390, 393, 392, 391, 397, 395, 398, 399, 394, 396, 400, 404, 403, 401, 402, 406, 408, 409, 407, 405, 410, 411, 413, 412, 417, 419, 418, 414, 416, 415, 420, 422, 424, 423, 425, 426, 421, 429, 427, 428, 430, 432, 433, 434, 431, 436, 439, 435, 437, 438, 440, 441, 443, 442, 448, 447, 445, 444, 449, 446, 450, 456, 455, 453, 454, 452, 451, 459, 457, 458, 460, 463, 461, 464, 462, 469, 467, 466, 468, 465, 470, 474, 471, 472, 473, 475, 477, 476, 479, 478, 480, 485, 481, 484, 482, 483, 486, 487, 488, 489, 490, 491, 493, 494, 492, 496, 495, 499, 498, 497, 500, 501, 502, 503, 504, 506, 507, 505, 509, 508, 510, 511, 512, 515, 514, 516, 513, 517, 519, 518, 521, 520, 523, 522, 524, 527, 526, 525, 528, 529, 530, 532, 531, 533, 535, 536, 537, 534, 539, 538, 540, 542, 541, 544, 545, 546, 547, 543, 548, 549, 550, 551, 553, 552, 559, 554, 558, 555, 557, 556, 560, 562, 563, 561, 564, 566, 565, 567, 568, 569, 570, 571, 572, 573, 575, 577, 574, 576, 579, 578, 580, 581, 582, 586, 584, 587, 585, 583, 589, 588, 590, 592, 593, 591, 594, 596, 597, 595, 599, 598, 600, 603, 601, 604, 602, 606, 605, 607, 609, 608, 610, 613, 612, 611, 614, 618, 616, 619, 617, 615, 620, 621, 623, 622, 624, 626, 629, 628, 625, 627, 630, 631, 633, 632, 635, 634, 636, 637, 638, 639, 640, 641, 643, 642, 645, 644, 649, 647, 648, 646, 650, 652, 653, 654, 651, 659, 657, 658, 656, 655, 660, 662, 663, 661, 667, 664, 666, 668, 665, 669, 670, 674, 672, 673, 671, 676, 677, 678, 679, 675, 680, 681, 684, 682, 685, 683, 687, 689, 686, 688, 690, 693, 692, 691, 698, 696, 695, 694, 697, 699, 700, 701, 702, 705, 703, 704, 707, 708, 709, 706, 710, 711, 712, 715, 714, 713, 718, 717, 719, 716, 720, 721, 723, 722, 724, 725, 726, 727, 728, 729, 730, 731, 733, 732, 735, 736, 734, 737, 739, 738, 741, 740, 742, 744, 743, 745, 748, 746, 747, 749, 750, 751, 752, 753, 756, 757, 755, 754, 758, 759, 760, 762, 763, 761, 767, 765, 764, 768, 766, 769, 770, 773, 772, 771, 774, 775, 776, 779, 777, 778, 780, 782, 781, 783, 784, 785, 786, 789, 787, 788, 790, 791, 792, 793, 796, 795, 794, 797, 798, 799, 800, 802, 801, 804, 803, 805, 806, 808, 807, 809, 810, 813, 811, 812, 814, 815, 818, 819, 817, 816, 820, 823, 822, 821, 825, 824, 827, 828, 829, 826, 830, 833, 836, 831, 832, 834, 835, 837, 838, 839, 840, 842, 841, 845, 844, 846, 843, 849, 848, 847, 850, 851, 852, 853, 854, 855, 857, 856, 858, 859, 860, 861, 862, 866, 864, 865, 863, 867, 868, 869, 870, 871, 877, 875, 876, 872, 874, 873, 878, 879, 880, 883, 882, 881, 888, 886, 889, 887, 885, 884, 890, 891, 892, 893, 894, 895, 896, 899, 897, 898, 900, 901, 902, 905, 904, 906, 903, 908, 907, 909, 910, 912, 913, 911, 914, 915, 916, 918, 919, 917, 920, 921, 924, 923, 922, 926, 925, 927, 928, 929, 930, 933, 932, 931, 935, 934, 936, 937, 939, 938, 940, 942, 941, 943, 948, 947, 945, 946, 944, 949, 950, 952, 951, 953, 954, 955, 956, 959, 957, 958, 960, 961, 962, 964, 965, 963, 969, 967, 968, 966, 970, 971, 972, 973, 974, 979, 978, 976, 975, 977, 980, 981, 982, 984, 986, 983, 985, 987, 988, 989, 990, 991, 992, 995, 994, 993, 998, 996, 999, 997, 210, 211, 218]\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","28\n","29\n","30\n","31\n","32\n","33\n","34\n","35\n","36\n","37\n","38\n","39\n","40\n","41\n","42\n","43\n","44\n","45\n","46\n","47\n","48\n","49\n","50\n","51\n","52\n","53\n","54\n","55\n","56\n","57\n","58\n","59\n","60\n","61\n","62\n","63\n","64\n","65\n","66\n","67\n","68\n","69\n","70\n","71\n","72\n","73\n","74\n","75\n","76\n","77\n","78\n","79\n","80\n","81\n","82\n","83\n","84\n","85\n","86\n","87\n","88\n","89\n","90\n","91\n","92\n","93\n","94\n","95\n","96\n","97\n","98\n","99\n","100\n","101\n","102\n","103\n","104\n","105\n","106\n","107\n","108\n","109\n","110\n","111\n","112\n","113\n","114\n","115\n","116\n","117\n","118\n","119\n","120\n","121\n","122\n","123\n","124\n","125\n","126\n","127\n","128\n","129\n","130\n","131\n","132\n","133\n","134\n","135\n","136\n","137\n","138\n","139\n","140\n","141\n","142\n","143\n","144\n","145\n","146\n","147\n","148\n","149\n","150\n","151\n","152\n","153\n","154\n","155\n","156\n","157\n","158\n","159\n","160\n","161\n","162\n","163\n","164\n","165\n","166\n","167\n","168\n","169\n","170\n","171\n","172\n","173\n","174\n","175\n","176\n","177\n","178\n","179\n","180\n","181\n","182\n","183\n","184\n","185\n","186\n","187\n","188\n","189\n","190\n","191\n","192\n","193\n","194\n","195\n","196\n","197\n","198\n","199\n","200\n","201\n","202\n","203\n","204\n","205\n","206\n","207\n","208\n","209\n","210\n","211\n","212\n","213\n","214\n","215\n","216\n","217\n","218\n","219\n","220\n","221\n","222\n","223\n","224\n","225\n","226\n","227\n","228\n","229\n","230\n","231\n","232\n","233\n","234\n","235\n","236\n","237\n","238\n","239\n","240\n","241\n","242\n","243\n","244\n","245\n","246\n","247\n","248\n","249\n","250\n","251\n","252\n","253\n","254\n","255\n","256\n","257\n","258\n","259\n","260\n","261\n","262\n","263\n","264\n","265\n","266\n","267\n","268\n","269\n","270\n","271\n","272\n","273\n","274\n","275\n","276\n","277\n","278\n","279\n","280\n","281\n","282\n","283\n","284\n","285\n","286\n","287\n","288\n","289\n","290\n","291\n","292\n","293\n","294\n","295\n","296\n","297\n","298\n","299\n","300\n","301\n","302\n","303\n","304\n","305\n","306\n","307\n","308\n","309\n","310\n","311\n","312\n","313\n","314\n","315\n","316\n","317\n","318\n","319\n","320\n","321\n","322\n","323\n","324\n","325\n","326\n","327\n","328\n","329\n","330\n","331\n","332\n","333\n","334\n","335\n","336\n","337\n","338\n","339\n","340\n","341\n","342\n","343\n","344\n","345\n","346\n","347\n","348\n","349\n","350\n","351\n","352\n","353\n","354\n","355\n","356\n","357\n","358\n","359\n","360\n","361\n","362\n","363\n","364\n","365\n","366\n","367\n","368\n","369\n","370\n","371\n","372\n","373\n","374\n","375\n","376\n","377\n","378\n","379\n","380\n","381\n","382\n","383\n","384\n","385\n","386\n","387\n","388\n","389\n","390\n","391\n","392\n","393\n","394\n","395\n","396\n","397\n","398\n","399\n","400\n","401\n","402\n","403\n","404\n","405\n","406\n","407\n","408\n","409\n","410\n","411\n","412\n","413\n","414\n","415\n","416\n","417\n","418\n","419\n","420\n","421\n","422\n","423\n","424\n","425\n","426\n","427\n","428\n","429\n","430\n","431\n","432\n","433\n","434\n","435\n","436\n","437\n","438\n","439\n","440\n","441\n","442\n","443\n","444\n","445\n","446\n","447\n","448\n","449\n","450\n","451\n","452\n","453\n","454\n","455\n","456\n","457\n","458\n","459\n","460\n","461\n","462\n","463\n","464\n","465\n","466\n","467\n","468\n","469\n","470\n","471\n","472\n","473\n","474\n","475\n","476\n","477\n","478\n","479\n","480\n","481\n","482\n","483\n","484\n","485\n","486\n","487\n","488\n","489\n","490\n","491\n","492\n","493\n","494\n","495\n","496\n","497\n","498\n","499\n","500\n","501\n","502\n","503\n","504\n","505\n","506\n","507\n","508\n","509\n","510\n","511\n","512\n","513\n","514\n","515\n","516\n","517\n","518\n","519\n","520\n","521\n","522\n","523\n","524\n","525\n","526\n","527\n","528\n","529\n","530\n","531\n","532\n","533\n","534\n","535\n","536\n","537\n","538\n","539\n","540\n","541\n","542\n","543\n","544\n","545\n","546\n","547\n","548\n","549\n","550\n","551\n","552\n","553\n","554\n","555\n","556\n","557\n","558\n","559\n","560\n","561\n","562\n","563\n","564\n","565\n","566\n","567\n","568\n","569\n","570\n","571\n","572\n","573\n","574\n","575\n","576\n","577\n","578\n","579\n","580\n","581\n","582\n","583\n","584\n","585\n","586\n","587\n","588\n","589\n","590\n","591\n","592\n","593\n","594\n","595\n","596\n","597\n","598\n","599\n","600\n","601\n","602\n","603\n","604\n","605\n","606\n","607\n","608\n","609\n","610\n","611\n","612\n","613\n","614\n","615\n","616\n","617\n","618\n","619\n","620\n","621\n","622\n","623\n","624\n","625\n","626\n","627\n","628\n","629\n","630\n","631\n","632\n","633\n","634\n","635\n","636\n","637\n","638\n","639\n","640\n","641\n","642\n","643\n","644\n","645\n","646\n","647\n","648\n","649\n","650\n","651\n","652\n","653\n","654\n","655\n","656\n","657\n","658\n","659\n","660\n","661\n","662\n","663\n","664\n","665\n","666\n","667\n","668\n","669\n","670\n","671\n","672\n","673\n","674\n","675\n","676\n","677\n","678\n","679\n","680\n","681\n","682\n","683\n","684\n","685\n","686\n","687\n","688\n","689\n","690\n","691\n","692\n","693\n","694\n","695\n","696\n","697\n","698\n","699\n","700\n","701\n","702\n","703\n","704\n","705\n","706\n","707\n","708\n","709\n","710\n","711\n","712\n","713\n","714\n","715\n","716\n","717\n","718\n","719\n","720\n","721\n","722\n","723\n","724\n","725\n","726\n","727\n","728\n","729\n","730\n","731\n","732\n","733\n","734\n","735\n","736\n","737\n","738\n","739\n","740\n","741\n","742\n","743\n","744\n","745\n","746\n","747\n","748\n","749\n","750\n","751\n","752\n","753\n","754\n","755\n","756\n","757\n","758\n","759\n","760\n","761\n","762\n","763\n","764\n","765\n","766\n","767\n","768\n","769\n","770\n","771\n","772\n","773\n","774\n","775\n","776\n","777\n","778\n","779\n","780\n","781\n","782\n","783\n","784\n","785\n","786\n","787\n","788\n","789\n","790\n","791\n","792\n","793\n","794\n","795\n","796\n","797\n","798\n","799\n","800\n","801\n","802\n","803\n","804\n","805\n","806\n","807\n","808\n","809\n","810\n","811\n","812\n","813\n","814\n","815\n","816\n","817\n","818\n","819\n","820\n","821\n","822\n","823\n","824\n","825\n","826\n","827\n","828\n","829\n","830\n","831\n","832\n","833\n","834\n","835\n","836\n","837\n","838\n","839\n","840\n","841\n","842\n","843\n","844\n","845\n","846\n","847\n","848\n","849\n","850\n","851\n","852\n","853\n","854\n","855\n","856\n","857\n","858\n","859\n","860\n","861\n","862\n","863\n","864\n","865\n","866\n","867\n","868\n","869\n","870\n","871\n","872\n","873\n","874\n","875\n","876\n","877\n","878\n","879\n","880\n","881\n","882\n","883\n","884\n","885\n","886\n","887\n","888\n","889\n","890\n","891\n","892\n","893\n","894\n","895\n","896\n","897\n","898\n","899\n","900\n","901\n","902\n","903\n","904\n","905\n","906\n","907\n","908\n","909\n","910\n","911\n","912\n","913\n","914\n","915\n","916\n","917\n","918\n","919\n","920\n","921\n","922\n","923\n","924\n","925\n","926\n","927\n","928\n","929\n","930\n","931\n","932\n","933\n","934\n","935\n","936\n","937\n","938\n","939\n","940\n","941\n","942\n","943\n","944\n","945\n","946\n","947\n","948\n","949\n","950\n","951\n","952\n","953\n","954\n","955\n","956\n","957\n","958\n","959\n","960\n","961\n","962\n","963\n","964\n","965\n","966\n","967\n","968\n","969\n","970\n","971\n","972\n","973\n","974\n","975\n","976\n","977\n","978\n","979\n","980\n","981\n","982\n","983\n","984\n","985\n","986\n","987\n","988\n","989\n","990\n","991\n","992\n","993\n","994\n","995\n","996\n","997\n","998\n","999\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"O5BW7NVuvmqN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":652},"outputId":"f16eb674-6499-4cf4-ae7b-04d4ce059351","executionInfo":{"status":"ok","timestamp":1570042074619,"user_tz":-330,"elapsed":1511,"user":{"displayName":"Jovian Jaison","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDD_VAnP3vWvojEh9tY5y5Xue8uiFD4JaqbNzzXjIs=s64","userId":"06808514560552349214"}}},"source":["from keras.models import Sequential, model_from_json\n","from keras.optimizers import SGD, RMSprop, Adam, Adagrad, Adadelta\n","from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, Conv2D, MaxPool2D, MaxPooling2D\n","from keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.utils.np_utils import to_categorical\n","\n","batch_size = 128\n","num_classes = 10\n","epochs = 20\n","img_rows,img_cols=256,256\n","input_shape = (img_rows, img_cols, 3)\n","e = 2\n","\n","model = Sequential()\n","model.add(Conv2D(32, kernel_size=(3, 3),\n","                 activation='relu',\n","                 input_shape=input_shape,strides=e))\n","model.add(Conv2D(64, (3, 3), activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Conv2D(128, kernel_size=(3, 3),\n","                 activation='relu'))\n","model.add(Conv2D(128, (3, 3), activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Conv2D(256, kernel_size=(3, 3),\n","                 activation='relu'))\n","model.add(Conv2D(512, (3, 3), activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))              \n","model.add(Dropout(0.25))\n","model.add(Flatten())\n","model.add(Dense(512, activation='relu'))\n","model.add(Dense(128, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(num_classes, activation='softmax'))\n","model.compile(loss=keras.losses.categorical_crossentropy,\n","              optimizer=Adadelta(),\n","              metrics=['acc'])\n","\n","model.summary()"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_7 (Conv2D)            (None, 127, 127, 32)      896       \n","_________________________________________________________________\n","conv2d_8 (Conv2D)            (None, 125, 125, 64)      18496     \n","_________________________________________________________________\n","max_pooling2d_4 (MaxPooling2 (None, 62, 62, 64)        0         \n","_________________________________________________________________\n","conv2d_9 (Conv2D)            (None, 60, 60, 128)       73856     \n","_________________________________________________________________\n","conv2d_10 (Conv2D)           (None, 58, 58, 128)       147584    \n","_________________________________________________________________\n","max_pooling2d_5 (MaxPooling2 (None, 29, 29, 128)       0         \n","_________________________________________________________________\n","conv2d_11 (Conv2D)           (None, 27, 27, 256)       295168    \n","_________________________________________________________________\n","conv2d_12 (Conv2D)           (None, 25, 25, 512)       1180160   \n","_________________________________________________________________\n","max_pooling2d_6 (MaxPooling2 (None, 12, 12, 512)       0         \n","_________________________________________________________________\n","dropout_3 (Dropout)          (None, 12, 12, 512)       0         \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 73728)             0         \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 512)               37749248  \n","_________________________________________________________________\n","dense_5 (Dense)              (None, 128)               65664     \n","_________________________________________________________________\n","dropout_4 (Dropout)          (None, 128)               0         \n","_________________________________________________________________\n","dense_6 (Dense)              (None, 10)                1290      \n","=================================================================\n","Total params: 39,532,362\n","Trainable params: 39,532,362\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pm9qEn7QvrlW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"9b735107-b6b1-4db4-baf7-02f2aee7e7d6","executionInfo":{"status":"ok","timestamp":1570045269191,"user_tz":-330,"elapsed":2728795,"user":{"displayName":"Jovian Jaison","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDD_VAnP3vWvojEh9tY5y5Xue8uiFD4JaqbNzzXjIs=s64","userId":"06808514560552349214"}}},"source":["datagen = ImageDataGenerator(\n","        featurewise_center=False,  # set input mean to 0 over the dataset\n","        samplewise_center=False,  # set each sample mean to 0\n","        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n","        samplewise_std_normalization=False,  # divide each input by its std\n","        zca_whitening=False,  # apply ZCA whitening\n","        rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n","        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n","        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n","        horizontal_flip=True,  # randomly flip images\n","        vertical_flip=True)  # randomly flip images\n","        \n","a = X_train\n","b = y_trainHot\n","c = X_test\n","d = y_testHot\n","epochs = 200\n","\n","history = model.fit_generator(datagen.flow(a,b, batch_size=32),\n","                        steps_per_epoch=len(a) / 32, \n","                              epochs=epochs,validation_data = [c, d])"],"execution_count":5,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","Epoch 1/200\n","25/25 [==============================] - 20s 793ms/step - loss: 2.3608 - acc: 0.0950 - val_loss: 2.2610 - val_acc: 0.1100\n","Epoch 2/200\n","25/25 [==============================] - 14s 542ms/step - loss: 2.2941 - acc: 0.1250 - val_loss: 2.2306 - val_acc: 0.1900\n","Epoch 3/200\n","25/25 [==============================] - 14s 556ms/step - loss: 2.1731 - acc: 0.1662 - val_loss: 2.0629 - val_acc: 0.1700\n","Epoch 4/200\n","25/25 [==============================] - 14s 554ms/step - loss: 2.1102 - acc: 0.2125 - val_loss: 2.0166 - val_acc: 0.2450\n","Epoch 5/200\n","25/25 [==============================] - 14s 553ms/step - loss: 2.1419 - acc: 0.1813 - val_loss: 2.0695 - val_acc: 0.1600\n","Epoch 6/200\n","25/25 [==============================] - 14s 550ms/step - loss: 2.0998 - acc: 0.2487 - val_loss: 1.8648 - val_acc: 0.3500\n","Epoch 7/200\n","25/25 [==============================] - 14s 545ms/step - loss: 2.0205 - acc: 0.2350 - val_loss: 1.8672 - val_acc: 0.3850\n","Epoch 8/200\n","25/25 [==============================] - 14s 545ms/step - loss: 1.9144 - acc: 0.2888 - val_loss: 1.6268 - val_acc: 0.4000\n","Epoch 9/200\n","25/25 [==============================] - 14s 554ms/step - loss: 1.8333 - acc: 0.3063 - val_loss: 1.5817 - val_acc: 0.4100\n","Epoch 10/200\n","25/25 [==============================] - 14s 545ms/step - loss: 1.8101 - acc: 0.3625 - val_loss: 1.7520 - val_acc: 0.3200\n","Epoch 11/200\n","25/25 [==============================] - 13s 535ms/step - loss: 1.6806 - acc: 0.3900 - val_loss: 1.2961 - val_acc: 0.5000\n","Epoch 12/200\n","25/25 [==============================] - 14s 542ms/step - loss: 1.5169 - acc: 0.4537 - val_loss: 1.4397 - val_acc: 0.4750\n","Epoch 13/200\n","25/25 [==============================] - 13s 536ms/step - loss: 1.3618 - acc: 0.5012 - val_loss: 1.1172 - val_acc: 0.5500\n","Epoch 14/200\n","25/25 [==============================] - 13s 538ms/step - loss: 1.4109 - acc: 0.4925 - val_loss: 1.1288 - val_acc: 0.5900\n","Epoch 15/200\n","25/25 [==============================] - 14s 541ms/step - loss: 1.3690 - acc: 0.4938 - val_loss: 1.0303 - val_acc: 0.5600\n","Epoch 16/200\n","25/25 [==============================] - 14s 541ms/step - loss: 1.2292 - acc: 0.5413 - val_loss: 1.0053 - val_acc: 0.5950\n","Epoch 17/200\n","25/25 [==============================] - 14s 554ms/step - loss: 1.2097 - acc: 0.5725 - val_loss: 1.0496 - val_acc: 0.5650\n","Epoch 18/200\n","25/25 [==============================] - 14s 551ms/step - loss: 1.2038 - acc: 0.5650 - val_loss: 1.0555 - val_acc: 0.5950\n","Epoch 19/200\n","25/25 [==============================] - 14s 555ms/step - loss: 1.1538 - acc: 0.6000 - val_loss: 0.8119 - val_acc: 0.6800\n","Epoch 20/200\n","25/25 [==============================] - 14s 549ms/step - loss: 1.0768 - acc: 0.6175 - val_loss: 0.9178 - val_acc: 0.6300\n","Epoch 21/200\n","25/25 [==============================] - 14s 544ms/step - loss: 1.0264 - acc: 0.6163 - val_loss: 0.9148 - val_acc: 0.6450\n","Epoch 22/200\n","25/25 [==============================] - 14s 549ms/step - loss: 1.0513 - acc: 0.6150 - val_loss: 0.8941 - val_acc: 0.6700\n","Epoch 23/200\n","25/25 [==============================] - 14s 549ms/step - loss: 1.1043 - acc: 0.6112 - val_loss: 0.8805 - val_acc: 0.6400\n","Epoch 24/200\n","25/25 [==============================] - 14s 550ms/step - loss: 1.0315 - acc: 0.6325 - val_loss: 0.8122 - val_acc: 0.6700\n","Epoch 25/200\n","25/25 [==============================] - 14s 546ms/step - loss: 0.9953 - acc: 0.6288 - val_loss: 0.8758 - val_acc: 0.6750\n","Epoch 26/200\n","25/25 [==============================] - 14s 550ms/step - loss: 1.0013 - acc: 0.6663 - val_loss: 1.0076 - val_acc: 0.6400\n","Epoch 27/200\n","25/25 [==============================] - 14s 542ms/step - loss: 1.0310 - acc: 0.6313 - val_loss: 0.7984 - val_acc: 0.6750\n","Epoch 28/200\n","25/25 [==============================] - 14s 548ms/step - loss: 0.9134 - acc: 0.6938 - val_loss: 0.6218 - val_acc: 0.7750\n","Epoch 29/200\n","25/25 [==============================] - 14s 552ms/step - loss: 0.8859 - acc: 0.6750 - val_loss: 0.6344 - val_acc: 0.7200\n","Epoch 30/200\n","25/25 [==============================] - 14s 544ms/step - loss: 0.9320 - acc: 0.6700 - val_loss: 0.6062 - val_acc: 0.7800\n","Epoch 31/200\n","25/25 [==============================] - 14s 540ms/step - loss: 0.8284 - acc: 0.6938 - val_loss: 0.9530 - val_acc: 0.6450\n","Epoch 32/200\n","25/25 [==============================] - 14s 540ms/step - loss: 0.8479 - acc: 0.6950 - val_loss: 0.8141 - val_acc: 0.7000\n","Epoch 33/200\n","25/25 [==============================] - 13s 537ms/step - loss: 0.8680 - acc: 0.7162 - val_loss: 0.5998 - val_acc: 0.7600\n","Epoch 34/200\n","25/25 [==============================] - 13s 535ms/step - loss: 0.8105 - acc: 0.7063 - val_loss: 0.7417 - val_acc: 0.7350\n","Epoch 35/200\n","25/25 [==============================] - 13s 535ms/step - loss: 0.8300 - acc: 0.7000 - val_loss: 0.7381 - val_acc: 0.7400\n","Epoch 36/200\n","25/25 [==============================] - 14s 547ms/step - loss: 0.7516 - acc: 0.7275 - val_loss: 0.6381 - val_acc: 0.7700\n","Epoch 37/200\n","25/25 [==============================] - 14s 546ms/step - loss: 0.7202 - acc: 0.7525 - val_loss: 0.5548 - val_acc: 0.7950\n","Epoch 38/200\n","25/25 [==============================] - 14s 552ms/step - loss: 0.7876 - acc: 0.7238 - val_loss: 0.5167 - val_acc: 0.7850\n","Epoch 39/200\n","25/25 [==============================] - 14s 546ms/step - loss: 0.6719 - acc: 0.7488 - val_loss: 0.6525 - val_acc: 0.7450\n","Epoch 40/200\n","25/25 [==============================] - 14s 545ms/step - loss: 0.6997 - acc: 0.7462 - val_loss: 0.5900 - val_acc: 0.7700\n","Epoch 41/200\n","25/25 [==============================] - 14s 548ms/step - loss: 0.6712 - acc: 0.7550 - val_loss: 0.5905 - val_acc: 0.7750\n","Epoch 42/200\n","25/25 [==============================] - 14s 550ms/step - loss: 0.6536 - acc: 0.7612 - val_loss: 0.6144 - val_acc: 0.7450\n","Epoch 43/200\n","25/25 [==============================] - 14s 544ms/step - loss: 0.6534 - acc: 0.7662 - val_loss: 0.5565 - val_acc: 0.7900\n","Epoch 44/200\n","25/25 [==============================] - 14s 548ms/step - loss: 0.6370 - acc: 0.7812 - val_loss: 1.0087 - val_acc: 0.7050\n","Epoch 45/200\n","25/25 [==============================] - 14s 548ms/step - loss: 0.7647 - acc: 0.7563 - val_loss: 0.6446 - val_acc: 0.7750\n","Epoch 46/200\n","25/25 [==============================] - 14s 547ms/step - loss: 0.5971 - acc: 0.7650 - val_loss: 0.5038 - val_acc: 0.7900\n","Epoch 47/200\n","25/25 [==============================] - 14s 545ms/step - loss: 0.6153 - acc: 0.7825 - val_loss: 0.6900 - val_acc: 0.7650\n","Epoch 48/200\n","25/25 [==============================] - 13s 536ms/step - loss: 0.7225 - acc: 0.7638 - val_loss: 0.4987 - val_acc: 0.8050\n","Epoch 49/200\n","25/25 [==============================] - 14s 549ms/step - loss: 0.5626 - acc: 0.8075 - val_loss: 1.6093 - val_acc: 0.6600\n","Epoch 50/200\n","25/25 [==============================] - 14s 542ms/step - loss: 0.6238 - acc: 0.7937 - val_loss: 0.7607 - val_acc: 0.7000\n","Epoch 51/200\n","25/25 [==============================] - 14s 548ms/step - loss: 0.5934 - acc: 0.8025 - val_loss: 0.5669 - val_acc: 0.8000\n","Epoch 52/200\n","25/25 [==============================] - 14s 541ms/step - loss: 0.4944 - acc: 0.8300 - val_loss: 0.4868 - val_acc: 0.8400\n","Epoch 53/200\n","25/25 [==============================] - 13s 540ms/step - loss: 0.4805 - acc: 0.8337 - val_loss: 0.5892 - val_acc: 0.7700\n","Epoch 54/200\n","25/25 [==============================] - 13s 538ms/step - loss: 0.5357 - acc: 0.8125 - val_loss: 0.5028 - val_acc: 0.8000\n","Epoch 55/200\n","25/25 [==============================] - 13s 537ms/step - loss: 0.4857 - acc: 0.8538 - val_loss: 0.4936 - val_acc: 0.8450\n","Epoch 56/200\n","25/25 [==============================] - 14s 547ms/step - loss: 0.4099 - acc: 0.8600 - val_loss: 0.8574 - val_acc: 0.7600\n","Epoch 57/200\n","25/25 [==============================] - 14s 542ms/step - loss: 0.5507 - acc: 0.8137 - val_loss: 0.4798 - val_acc: 0.8350\n","Epoch 58/200\n","25/25 [==============================] - 14s 548ms/step - loss: 0.4979 - acc: 0.8300 - val_loss: 0.5561 - val_acc: 0.8300\n","Epoch 59/200\n","25/25 [==============================] - 13s 539ms/step - loss: 0.6014 - acc: 0.8137 - val_loss: 0.4969 - val_acc: 0.8350\n","Epoch 60/200\n","25/25 [==============================] - 13s 538ms/step - loss: 0.4614 - acc: 0.8387 - val_loss: 0.4308 - val_acc: 0.8450\n","Epoch 61/200\n","25/25 [==============================] - 13s 535ms/step - loss: 0.3988 - acc: 0.8550 - val_loss: 0.4560 - val_acc: 0.8500\n","Epoch 62/200\n","25/25 [==============================] - 13s 535ms/step - loss: 0.4791 - acc: 0.8588 - val_loss: 0.4050 - val_acc: 0.8650\n","Epoch 63/200\n","25/25 [==============================] - 13s 535ms/step - loss: 0.3773 - acc: 0.8700 - val_loss: 0.4386 - val_acc: 0.8550\n","Epoch 64/200\n","25/25 [==============================] - 14s 541ms/step - loss: 0.4838 - acc: 0.8425 - val_loss: 0.5971 - val_acc: 0.8200\n","Epoch 65/200\n","25/25 [==============================] - 14s 543ms/step - loss: 0.4690 - acc: 0.8300 - val_loss: 0.5363 - val_acc: 0.8150\n","Epoch 66/200\n","25/25 [==============================] - 14s 548ms/step - loss: 0.4208 - acc: 0.8612 - val_loss: 0.4683 - val_acc: 0.8400\n","Epoch 67/200\n","25/25 [==============================] - 14s 544ms/step - loss: 0.4356 - acc: 0.8600 - val_loss: 0.5874 - val_acc: 0.8000\n","Epoch 68/200\n","25/25 [==============================] - 13s 538ms/step - loss: 0.3878 - acc: 0.8800 - val_loss: 0.3815 - val_acc: 0.8550\n","Epoch 69/200\n","25/25 [==============================] - 14s 541ms/step - loss: 0.3779 - acc: 0.8687 - val_loss: 0.5444 - val_acc: 0.8200\n","Epoch 70/200\n","25/25 [==============================] - 14s 541ms/step - loss: 0.3446 - acc: 0.8788 - val_loss: 0.4642 - val_acc: 0.8350\n","Epoch 71/200\n","25/25 [==============================] - 14s 541ms/step - loss: 0.4451 - acc: 0.8500 - val_loss: 0.4483 - val_acc: 0.8450\n","Epoch 72/200\n","25/25 [==============================] - 14s 540ms/step - loss: 0.3403 - acc: 0.9000 - val_loss: 0.4684 - val_acc: 0.8500\n","Epoch 73/200\n","25/25 [==============================] - 13s 535ms/step - loss: 0.3558 - acc: 0.8925 - val_loss: 0.3684 - val_acc: 0.8650\n","Epoch 74/200\n","25/25 [==============================] - 13s 539ms/step - loss: 0.4622 - acc: 0.8463 - val_loss: 0.6243 - val_acc: 0.8150\n","Epoch 75/200\n","25/25 [==============================] - 14s 545ms/step - loss: 0.4908 - acc: 0.8712 - val_loss: 0.4578 - val_acc: 0.8400\n","Epoch 76/200\n","25/25 [==============================] - 13s 540ms/step - loss: 0.3352 - acc: 0.8738 - val_loss: 0.4178 - val_acc: 0.8750\n","Epoch 77/200\n","25/25 [==============================] - 13s 536ms/step - loss: 0.2468 - acc: 0.9113 - val_loss: 0.5390 - val_acc: 0.8300\n","Epoch 78/200\n","25/25 [==============================] - 13s 538ms/step - loss: 0.4620 - acc: 0.8650 - val_loss: 0.5441 - val_acc: 0.8100\n","Epoch 79/200\n","25/25 [==============================] - 13s 537ms/step - loss: 0.2765 - acc: 0.9075 - val_loss: 0.3717 - val_acc: 0.8750\n","Epoch 80/200\n","25/25 [==============================] - 13s 538ms/step - loss: 0.3101 - acc: 0.8950 - val_loss: 0.4854 - val_acc: 0.8600\n","Epoch 81/200\n","25/25 [==============================] - 13s 537ms/step - loss: 0.2783 - acc: 0.9037 - val_loss: 0.4172 - val_acc: 0.8550\n","Epoch 82/200\n","25/25 [==============================] - 13s 538ms/step - loss: 0.3458 - acc: 0.8962 - val_loss: 0.3442 - val_acc: 0.8800\n","Epoch 83/200\n","25/25 [==============================] - 14s 544ms/step - loss: 0.3060 - acc: 0.9037 - val_loss: 0.6726 - val_acc: 0.8300\n","Epoch 84/200\n","25/25 [==============================] - 14s 545ms/step - loss: 0.2506 - acc: 0.9138 - val_loss: 0.3953 - val_acc: 0.8800\n","Epoch 85/200\n","25/25 [==============================] - 13s 538ms/step - loss: 0.2774 - acc: 0.9075 - val_loss: 0.3750 - val_acc: 0.8700\n","Epoch 86/200\n","25/25 [==============================] - 14s 545ms/step - loss: 0.5999 - acc: 0.8237 - val_loss: 0.3590 - val_acc: 0.8400\n","Epoch 87/200\n","25/25 [==============================] - 14s 550ms/step - loss: 0.2374 - acc: 0.9050 - val_loss: 0.3154 - val_acc: 0.8950\n","Epoch 88/200\n","25/25 [==============================] - 14s 548ms/step - loss: 0.1924 - acc: 0.9300 - val_loss: 0.3649 - val_acc: 0.8800\n","Epoch 89/200\n","25/25 [==============================] - 14s 550ms/step - loss: 0.3111 - acc: 0.8925 - val_loss: 0.6165 - val_acc: 0.8100\n","Epoch 90/200\n","25/25 [==============================] - 14s 552ms/step - loss: 0.3440 - acc: 0.8837 - val_loss: 0.4772 - val_acc: 0.8500\n","Epoch 91/200\n","25/25 [==============================] - 14s 556ms/step - loss: 0.2382 - acc: 0.9113 - val_loss: 0.4780 - val_acc: 0.8550\n","Epoch 92/200\n","25/25 [==============================] - 14s 552ms/step - loss: 0.3090 - acc: 0.9062 - val_loss: 0.3898 - val_acc: 0.8650\n","Epoch 93/200\n","25/25 [==============================] - 14s 548ms/step - loss: 0.3131 - acc: 0.8987 - val_loss: 0.3946 - val_acc: 0.8700\n","Epoch 94/200\n","25/25 [==============================] - 14s 547ms/step - loss: 0.2440 - acc: 0.9150 - val_loss: 0.3791 - val_acc: 0.8850\n","Epoch 95/200\n","25/25 [==============================] - 14s 548ms/step - loss: 0.2484 - acc: 0.9113 - val_loss: 0.4594 - val_acc: 0.8700\n","Epoch 96/200\n","25/25 [==============================] - 14s 550ms/step - loss: 0.2070 - acc: 0.9213 - val_loss: 1.0054 - val_acc: 0.7750\n","Epoch 97/200\n","25/25 [==============================] - 14s 553ms/step - loss: 0.3301 - acc: 0.8938 - val_loss: 0.4854 - val_acc: 0.8650\n","Epoch 98/200\n","25/25 [==============================] - 14s 554ms/step - loss: 0.2423 - acc: 0.9237 - val_loss: 0.4043 - val_acc: 0.8750\n","Epoch 99/200\n","25/25 [==============================] - 14s 548ms/step - loss: 0.2309 - acc: 0.9263 - val_loss: 0.4819 - val_acc: 0.8450\n","Epoch 100/200\n","25/25 [==============================] - 14s 549ms/step - loss: 0.3514 - acc: 0.8938 - val_loss: 0.4165 - val_acc: 0.8500\n","Epoch 101/200\n","25/25 [==============================] - 14s 545ms/step - loss: 0.2519 - acc: 0.9150 - val_loss: 0.7194 - val_acc: 0.7850\n","Epoch 102/200\n","25/25 [==============================] - 14s 544ms/step - loss: 0.2479 - acc: 0.9150 - val_loss: 0.5700 - val_acc: 0.8200\n","Epoch 103/200\n","25/25 [==============================] - 14s 546ms/step - loss: 0.3245 - acc: 0.8975 - val_loss: 0.4252 - val_acc: 0.8800\n","Epoch 104/200\n","25/25 [==============================] - 14s 551ms/step - loss: 0.1886 - acc: 0.9413 - val_loss: 0.5305 - val_acc: 0.8400\n","Epoch 105/200\n","25/25 [==============================] - 14s 546ms/step - loss: 0.2460 - acc: 0.9113 - val_loss: 0.6499 - val_acc: 0.8200\n","Epoch 106/200\n","25/25 [==============================] - 14s 550ms/step - loss: 0.2363 - acc: 0.9050 - val_loss: 0.6050 - val_acc: 0.8300\n","Epoch 107/200\n","25/25 [==============================] - 14s 551ms/step - loss: 0.1903 - acc: 0.9250 - val_loss: 0.4658 - val_acc: 0.8450\n","Epoch 108/200\n","25/25 [==============================] - 14s 550ms/step - loss: 0.1966 - acc: 0.9288 - val_loss: 0.4144 - val_acc: 0.8700\n","Epoch 109/200\n","25/25 [==============================] - 14s 543ms/step - loss: 0.1954 - acc: 0.9337 - val_loss: 0.7186 - val_acc: 0.8400\n","Epoch 110/200\n","25/25 [==============================] - 14s 551ms/step - loss: 0.3210 - acc: 0.9012 - val_loss: 0.4101 - val_acc: 0.8800\n","Epoch 111/200\n","25/25 [==============================] - 14s 550ms/step - loss: 0.1946 - acc: 0.9350 - val_loss: 0.3944 - val_acc: 0.8650\n","Epoch 112/200\n","25/25 [==============================] - 14s 548ms/step - loss: 0.2009 - acc: 0.9387 - val_loss: 0.6303 - val_acc: 0.8200\n","Epoch 113/200\n","25/25 [==============================] - 14s 551ms/step - loss: 0.4811 - acc: 0.9150 - val_loss: 0.6605 - val_acc: 0.8100\n","Epoch 114/200\n","25/25 [==============================] - 14s 549ms/step - loss: 0.2273 - acc: 0.9312 - val_loss: 0.7463 - val_acc: 0.8250\n","Epoch 115/200\n","25/25 [==============================] - 14s 549ms/step - loss: 0.1629 - acc: 0.9438 - val_loss: 0.4025 - val_acc: 0.8950\n","Epoch 116/200\n","25/25 [==============================] - 14s 547ms/step - loss: 0.1458 - acc: 0.9513 - val_loss: 0.4550 - val_acc: 0.8750\n","Epoch 117/200\n","25/25 [==============================] - 14s 545ms/step - loss: 0.1807 - acc: 0.9375 - val_loss: 0.4295 - val_acc: 0.8900\n","Epoch 118/200\n","25/25 [==============================] - 14s 545ms/step - loss: 0.2202 - acc: 0.9400 - val_loss: 0.4379 - val_acc: 0.8850\n","Epoch 119/200\n","25/25 [==============================] - 14s 548ms/step - loss: 0.1654 - acc: 0.9450 - val_loss: 0.5641 - val_acc: 0.8600\n","Epoch 120/200\n","25/25 [==============================] - 13s 538ms/step - loss: 0.4113 - acc: 0.9037 - val_loss: 0.4306 - val_acc: 0.8700\n","Epoch 121/200\n","25/25 [==============================] - 14s 543ms/step - loss: 0.1692 - acc: 0.9413 - val_loss: 0.4993 - val_acc: 0.8850\n","Epoch 122/200\n","25/25 [==============================] - 14s 544ms/step - loss: 0.2273 - acc: 0.9300 - val_loss: 0.4791 - val_acc: 0.8750\n","Epoch 123/200\n","25/25 [==============================] - 14s 556ms/step - loss: 0.2195 - acc: 0.9425 - val_loss: 0.3571 - val_acc: 0.8750\n","Epoch 124/200\n","25/25 [==============================] - 14s 552ms/step - loss: 0.1413 - acc: 0.9462 - val_loss: 0.3988 - val_acc: 0.9000\n","Epoch 125/200\n","25/25 [==============================] - 14s 550ms/step - loss: 0.1342 - acc: 0.9450 - val_loss: 0.5134 - val_acc: 0.8550\n","Epoch 126/200\n","25/25 [==============================] - 14s 545ms/step - loss: 0.2082 - acc: 0.9387 - val_loss: 0.5564 - val_acc: 0.8850\n","Epoch 127/200\n","25/25 [==============================] - 14s 553ms/step - loss: 0.3078 - acc: 0.8987 - val_loss: 0.5620 - val_acc: 0.8700\n","Epoch 128/200\n","25/25 [==============================] - 14s 553ms/step - loss: 0.1708 - acc: 0.9500 - val_loss: 0.6057 - val_acc: 0.8450\n","Epoch 129/200\n","25/25 [==============================] - 14s 549ms/step - loss: 0.2395 - acc: 0.9288 - val_loss: 0.3621 - val_acc: 0.8900\n","Epoch 130/200\n","25/25 [==============================] - 14s 552ms/step - loss: 0.1929 - acc: 0.9500 - val_loss: 0.8376 - val_acc: 0.8450\n","Epoch 131/200\n","25/25 [==============================] - 14s 544ms/step - loss: 0.1757 - acc: 0.9413 - val_loss: 0.6257 - val_acc: 0.8650\n","Epoch 132/200\n","25/25 [==============================] - 14s 548ms/step - loss: 0.1091 - acc: 0.9637 - val_loss: 0.5473 - val_acc: 0.8950\n","Epoch 133/200\n","25/25 [==============================] - 14s 541ms/step - loss: 0.1014 - acc: 0.9738 - val_loss: 0.4618 - val_acc: 0.8800\n","Epoch 134/200\n","25/25 [==============================] - 14s 543ms/step - loss: 0.1693 - acc: 0.9413 - val_loss: 0.7108 - val_acc: 0.8700\n","Epoch 135/200\n","25/25 [==============================] - 14s 543ms/step - loss: 0.1987 - acc: 0.9312 - val_loss: 0.4880 - val_acc: 0.8800\n","Epoch 136/200\n","25/25 [==============================] - 14s 548ms/step - loss: 0.1012 - acc: 0.9600 - val_loss: 0.4996 - val_acc: 0.8850\n","Epoch 137/200\n","25/25 [==============================] - 14s 552ms/step - loss: 0.3036 - acc: 0.9213 - val_loss: 0.5247 - val_acc: 0.8450\n","Epoch 138/200\n","25/25 [==============================] - 14s 548ms/step - loss: 0.3326 - acc: 0.9037 - val_loss: 0.5104 - val_acc: 0.8450\n","Epoch 139/200\n","25/25 [==============================] - 14s 552ms/step - loss: 0.1031 - acc: 0.9637 - val_loss: 0.4456 - val_acc: 0.8700\n","Epoch 140/200\n","25/25 [==============================] - 14s 552ms/step - loss: 0.0963 - acc: 0.9675 - val_loss: 0.5423 - val_acc: 0.8800\n","Epoch 141/200\n","25/25 [==============================] - 14s 548ms/step - loss: 0.1914 - acc: 0.9350 - val_loss: 0.8150 - val_acc: 0.8350\n","Epoch 142/200\n","25/25 [==============================] - 14s 552ms/step - loss: 0.1632 - acc: 0.9450 - val_loss: 0.5567 - val_acc: 0.8550\n","Epoch 143/200\n","25/25 [==============================] - 14s 547ms/step - loss: 0.3187 - acc: 0.9312 - val_loss: 0.8236 - val_acc: 0.7950\n","Epoch 144/200\n","25/25 [==============================] - 14s 552ms/step - loss: 0.1140 - acc: 0.9675 - val_loss: 0.3828 - val_acc: 0.8950\n","Epoch 145/200\n","25/25 [==============================] - 14s 549ms/step - loss: 0.1290 - acc: 0.9600 - val_loss: 0.4363 - val_acc: 0.8700\n","Epoch 146/200\n","25/25 [==============================] - 14s 547ms/step - loss: 0.1600 - acc: 0.9475 - val_loss: 0.4163 - val_acc: 0.9000\n","Epoch 147/200\n","25/25 [==============================] - 14s 548ms/step - loss: 0.1870 - acc: 0.9425 - val_loss: 0.3972 - val_acc: 0.8800\n","Epoch 148/200\n","25/25 [==============================] - 14s 553ms/step - loss: 0.0755 - acc: 0.9775 - val_loss: 0.5834 - val_acc: 0.8700\n","Epoch 149/200\n","25/25 [==============================] - 14s 547ms/step - loss: 0.2721 - acc: 0.9187 - val_loss: 0.5077 - val_acc: 0.8650\n","Epoch 150/200\n","25/25 [==============================] - 14s 547ms/step - loss: 0.1077 - acc: 0.9637 - val_loss: 0.3866 - val_acc: 0.8900\n","Epoch 151/200\n","25/25 [==============================] - 14s 544ms/step - loss: 0.0837 - acc: 0.9738 - val_loss: 0.4113 - val_acc: 0.8850\n","Epoch 152/200\n","25/25 [==============================] - 14s 542ms/step - loss: 0.1506 - acc: 0.9575 - val_loss: 0.4802 - val_acc: 0.8850\n","Epoch 153/200\n","25/25 [==============================] - 13s 538ms/step - loss: 0.0818 - acc: 0.9700 - val_loss: 0.5897 - val_acc: 0.8700\n","Epoch 154/200\n","25/25 [==============================] - 14s 544ms/step - loss: 0.0584 - acc: 0.9850 - val_loss: 0.8236 - val_acc: 0.8550\n","Epoch 155/200\n","25/25 [==============================] - 14s 543ms/step - loss: 0.2232 - acc: 0.9400 - val_loss: 0.4850 - val_acc: 0.8800\n","Epoch 156/200\n","25/25 [==============================] - 13s 539ms/step - loss: 0.1351 - acc: 0.9513 - val_loss: 0.5656 - val_acc: 0.9050\n","Epoch 157/200\n","25/25 [==============================] - 13s 535ms/step - loss: 0.1276 - acc: 0.9575 - val_loss: 0.7270 - val_acc: 0.8750\n","Epoch 158/200\n","25/25 [==============================] - 13s 537ms/step - loss: 0.2023 - acc: 0.9525 - val_loss: 0.7012 - val_acc: 0.8250\n","Epoch 159/200\n","25/25 [==============================] - 13s 528ms/step - loss: 0.1088 - acc: 0.9675 - val_loss: 0.4750 - val_acc: 0.8600\n","Epoch 160/200\n","25/25 [==============================] - 13s 538ms/step - loss: 0.3033 - acc: 0.9387 - val_loss: 0.6253 - val_acc: 0.8500\n","Epoch 161/200\n","25/25 [==============================] - 13s 536ms/step - loss: 0.1077 - acc: 0.9600 - val_loss: 0.4115 - val_acc: 0.8950\n","Epoch 162/200\n","25/25 [==============================] - 13s 532ms/step - loss: 0.0924 - acc: 0.9662 - val_loss: 0.3849 - val_acc: 0.9050\n","Epoch 163/200\n","25/25 [==============================] - 13s 537ms/step - loss: 0.0714 - acc: 0.9812 - val_loss: 0.4462 - val_acc: 0.8900\n","Epoch 164/200\n","25/25 [==============================] - 13s 539ms/step - loss: 0.1580 - acc: 0.9450 - val_loss: 0.4256 - val_acc: 0.9100\n","Epoch 165/200\n","25/25 [==============================] - 13s 530ms/step - loss: 0.0929 - acc: 0.9713 - val_loss: 0.4969 - val_acc: 0.8950\n","Epoch 166/200\n","25/25 [==============================] - 13s 535ms/step - loss: 0.1190 - acc: 0.9650 - val_loss: 1.0422 - val_acc: 0.7900\n","Epoch 167/200\n","25/25 [==============================] - 13s 539ms/step - loss: 0.1646 - acc: 0.9563 - val_loss: 0.4621 - val_acc: 0.9000\n","Epoch 168/200\n","25/25 [==============================] - 13s 537ms/step - loss: 0.1396 - acc: 0.9550 - val_loss: 0.5245 - val_acc: 0.8800\n","Epoch 169/200\n","25/25 [==============================] - 13s 540ms/step - loss: 0.1124 - acc: 0.9700 - val_loss: 0.4239 - val_acc: 0.8850\n","Epoch 170/200\n","25/25 [==============================] - 13s 536ms/step - loss: 0.0543 - acc: 0.9788 - val_loss: 0.5616 - val_acc: 0.9000\n","Epoch 171/200\n","25/25 [==============================] - 13s 533ms/step - loss: 0.1480 - acc: 0.9525 - val_loss: 0.6027 - val_acc: 0.8600\n","Epoch 172/200\n","25/25 [==============================] - 13s 534ms/step - loss: 0.1540 - acc: 0.9575 - val_loss: 0.5571 - val_acc: 0.8500\n","Epoch 173/200\n","25/25 [==============================] - 14s 542ms/step - loss: 0.0882 - acc: 0.9700 - val_loss: 0.4730 - val_acc: 0.8950\n","Epoch 174/200\n","25/25 [==============================] - 13s 538ms/step - loss: 0.1267 - acc: 0.9563 - val_loss: 0.6128 - val_acc: 0.8750\n","Epoch 175/200\n","25/25 [==============================] - 13s 538ms/step - loss: 0.0626 - acc: 0.9825 - val_loss: 0.6653 - val_acc: 0.9000\n","Epoch 176/200\n","25/25 [==============================] - 13s 536ms/step - loss: 0.1151 - acc: 0.9750 - val_loss: 0.5667 - val_acc: 0.8900\n","Epoch 177/200\n","25/25 [==============================] - 13s 536ms/step - loss: 0.1419 - acc: 0.9587 - val_loss: 0.4760 - val_acc: 0.8850\n","Epoch 178/200\n","25/25 [==============================] - 13s 539ms/step - loss: 0.0722 - acc: 0.9800 - val_loss: 0.5257 - val_acc: 0.8950\n","Epoch 179/200\n","25/25 [==============================] - 13s 533ms/step - loss: 0.1683 - acc: 0.9575 - val_loss: 0.5019 - val_acc: 0.8950\n","Epoch 180/200\n","25/25 [==============================] - 13s 535ms/step - loss: 0.0728 - acc: 0.9763 - val_loss: 0.5748 - val_acc: 0.9050\n","Epoch 181/200\n","25/25 [==============================] - 13s 535ms/step - loss: 0.0450 - acc: 0.9850 - val_loss: 0.8380 - val_acc: 0.8350\n","Epoch 182/200\n","25/25 [==============================] - 13s 537ms/step - loss: 0.1515 - acc: 0.9550 - val_loss: 0.9739 - val_acc: 0.8550\n","Epoch 183/200\n","25/25 [==============================] - 13s 539ms/step - loss: 0.1359 - acc: 0.9763 - val_loss: 0.5874 - val_acc: 0.8950\n","Epoch 184/200\n","25/25 [==============================] - 13s 536ms/step - loss: 0.1561 - acc: 0.9612 - val_loss: 0.7085 - val_acc: 0.8400\n","Epoch 185/200\n","25/25 [==============================] - 14s 543ms/step - loss: 0.0995 - acc: 0.9675 - val_loss: 0.4857 - val_acc: 0.9100\n","Epoch 186/200\n","25/25 [==============================] - 14s 544ms/step - loss: 0.0718 - acc: 0.9775 - val_loss: 0.6644 - val_acc: 0.9050\n","Epoch 187/200\n","25/25 [==============================] - 13s 538ms/step - loss: 0.2119 - acc: 0.9462 - val_loss: 0.4512 - val_acc: 0.9100\n","Epoch 188/200\n","25/25 [==============================] - 14s 543ms/step - loss: 0.0842 - acc: 0.9800 - val_loss: 0.5172 - val_acc: 0.9150\n","Epoch 189/200\n","25/25 [==============================] - 14s 546ms/step - loss: 0.0878 - acc: 0.9738 - val_loss: 0.7698 - val_acc: 0.8850\n","Epoch 190/200\n","25/25 [==============================] - 14s 540ms/step - loss: 0.1693 - acc: 0.9537 - val_loss: 0.6964 - val_acc: 0.8800\n","Epoch 191/200\n","25/25 [==============================] - 13s 539ms/step - loss: 0.1270 - acc: 0.9637 - val_loss: 0.5170 - val_acc: 0.9200\n","Epoch 192/200\n","25/25 [==============================] - 13s 538ms/step - loss: 0.0518 - acc: 0.9838 - val_loss: 0.6636 - val_acc: 0.8700\n","Epoch 193/200\n","25/25 [==============================] - 13s 533ms/step - loss: 0.2341 - acc: 0.9537 - val_loss: 0.8861 - val_acc: 0.8650\n","Epoch 194/200\n","25/25 [==============================] - 13s 540ms/step - loss: 0.1399 - acc: 0.9650 - val_loss: 2.0355 - val_acc: 0.7800\n","Epoch 195/200\n","25/25 [==============================] - 14s 542ms/step - loss: 0.0789 - acc: 0.9763 - val_loss: 0.4570 - val_acc: 0.9000\n","Epoch 196/200\n","25/25 [==============================] - 13s 537ms/step - loss: 0.0484 - acc: 0.9812 - val_loss: 0.5667 - val_acc: 0.9000\n","Epoch 197/200\n","25/25 [==============================] - 13s 538ms/step - loss: 0.1459 - acc: 0.9575 - val_loss: 1.1179 - val_acc: 0.8500\n","Epoch 198/200\n","25/25 [==============================] - 13s 537ms/step - loss: 0.1736 - acc: 0.9500 - val_loss: 0.6049 - val_acc: 0.8800\n","Epoch 199/200\n","25/25 [==============================] - 13s 535ms/step - loss: 0.1210 - acc: 0.9675 - val_loss: 0.9113 - val_acc: 0.8150\n","Epoch 200/200\n","25/25 [==============================] - 13s 534ms/step - loss: 0.1518 - acc: 0.9600 - val_loss: 0.5673 - val_acc: 0.8750\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"V3MlrnkU_Gpe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"d83af846-50be-40a6-9bd5-2e793525cba0","executionInfo":{"status":"ok","timestamp":1570045279505,"user_tz":-330,"elapsed":4561,"user":{"displayName":"Jovian Jaison","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDD_VAnP3vWvojEh9tY5y5Xue8uiFD4JaqbNzzXjIs=s64","userId":"06808514560552349214"}}},"source":["from keras.models import Model\n","intermediate_layer_model = Model(inputs=model.input,\n","                                 outputs=model.get_layer('dense_4').output)\n","intermediate_output = intermediate_layer_model.predict(X)\n","import pandas as pd\n","print(intermediate_output.shape)\n","df  = pd.DataFrame(intermediate_output)\n","print(df)\n","df.to_csv('3xCNN.csv')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["(1000, 512)\n","     0    1         2    3    4    5    ...  506  507  508       509        510  511\n","0    0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   1.895543  0.0\n","1    0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   0.000000  0.0\n","2    0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   0.000000  0.0\n","3    0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   0.000000  0.0\n","4    0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   0.974553  0.0\n","5    0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   0.524511  0.0\n","6    0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   1.423036  0.0\n","7    0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   0.456119  0.0\n","8    0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   2.334197  0.0\n","9    0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   1.914673  0.0\n","10   0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   2.254638  0.0\n","11   0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   1.359874  0.0\n","12   0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   3.347375  0.0\n","13   0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   3.207803  0.0\n","14   0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   0.000000  0.0\n","15   0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   6.907312  0.0\n","16   0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   2.447089  0.0\n","17   0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   0.006358  0.0\n","18   0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   0.000000  0.0\n","19   0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   0.000000  0.0\n","20   0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   2.416983  0.0\n","21   0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   0.788766  0.0\n","22   0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   0.511301  0.0\n","23   0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   1.347450  0.0\n","24   0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   3.205596  0.0\n","25   0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   0.000000  0.0\n","26   0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   4.542553  0.0\n","27   0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   0.000000  0.0\n","28   0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   0.195499  0.0\n","29   0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   0.492369  0.0\n","..   ...  ...       ...  ...  ...  ...  ...  ...  ...  ...       ...        ...  ...\n","970  0.0  0.0  1.820762  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   8.578732  0.0\n","971  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   1.563438  0.0\n","972  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   5.592608  0.0\n","973  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000  19.151953  0.0\n","974  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   8.258412  0.0\n","975  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   8.797248  0.0\n","976  0.0  0.0  1.296995  0.0  0.0  0.0  ...  0.0  0.0  0.0  1.508093  10.562129  0.0\n","977  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000  13.409095  0.0\n","978  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000  11.921970  0.0\n","979  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   5.064144  0.0\n","980  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000  22.429417  0.0\n","981  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000  20.535494  0.0\n","982  0.0  0.0  0.108940  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000  10.606430  0.0\n","983  0.0  0.0  0.416559  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.030673   6.836105  0.0\n","984  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   7.853007  0.0\n","985  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   8.892275  0.0\n","986  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000  11.877804  0.0\n","987  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   5.542555  0.0\n","988  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   9.965975  0.0\n","989  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000  11.100104  0.0\n","990  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   7.776829  0.0\n","991  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000  14.173508  0.0\n","992  0.0  0.0  1.735600  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000  12.198455  0.0\n","993  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.938746  15.505908  0.0\n","994  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000  15.051597  0.0\n","995  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   4.694876  0.0\n","996  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   4.799450  0.0\n","997  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   7.291370  0.0\n","998  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   4.995188  0.0\n","999  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000   3.437371  0.0\n","\n","[1000 rows x 512 columns]\n"],"name":"stdout"}]}]}